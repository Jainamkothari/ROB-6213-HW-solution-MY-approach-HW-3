{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd759464-ca2b-45e5-a5af-cd4cbbd6ae19",
   "metadata": {},
   "source": [
    "# I have added the comments for better understanding and tried to write the code in similar way as we are taught in theory in class first cell is 2 A answer and second cell is 2 B answer ( i have used different methods for each and called with different variables in both as i was having certain problems and to solve them i did so ) , third cell is 2 C answer now give this in python markdown code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b914a63-7f35-49cb-ad6d-e82d946d651c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration Converged in: 1376 iterations\n",
      "\n",
      "Final Value Function after Value Iteration (Cost-to-Go for Each Cell):\n",
      "[['-95.07940236574548' '-96.03980136574546' '-97.00990136574546'\n",
      "  '-98.99990136574546' '-99.99990136574546']\n",
      " ['-94.12860735574546' '0.0' '-96.02980136574546' '-98.00990136574546'\n",
      "  '-88.99990136574546']\n",
      " ['-93.18732029584545' '-82.25544610654447' '0.0' '-97.00990136574546'\n",
      "  '-98.99990136574546']\n",
      " ['-91.25544610654447' '-90.34289065913646' '-89.43946076620256'\n",
      "  '-88.99990136574546' '-99.99990136574546']]\n",
      "\n",
      "Final Policy (Best Action at Each Cell):\n",
      "[['right' 'right' 'right' 'right' 'stay']\n",
      " ['up' None 'right' 'up' 'up']\n",
      " ['up' 'left' None 'right' 'down']\n",
      " ['up' 'left' 'left' 'right' 'stay']]\n"
     ]
    }
   ],
   "source": [
    "#2 A )\n",
    "\n",
    "\n",
    "# Aswwer\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define grid world dimensions and cell types\n",
    "rows, cols = 4, 5\n",
    "alpha = 0.99  # Discount factor\n",
    "convergence_threshold = 1e-6  # Convergence threshold\n",
    "\n",
    "# Define costs based on cell colors\n",
    "costs = {\n",
    "    \"violet\": -1,\n",
    "    \"white\": 0,\n",
    "    \"green\": 1,\n",
    "    \"red\": 10,\n",
    "    \"gray\": None  # gray cells are obstacles\n",
    "}\n",
    "\n",
    "# Create the grid with costs based on the given figure\n",
    "grid = [\n",
    "    [\"white\", \"white\", \"green\", \"white\", \"violet\"],\n",
    "    [\"white\", \"gray\", \"green\", \"white\", \"red\"],\n",
    "    [\"white\", \"red\", \"gray\", \"green\", \"white\"],\n",
    "    [\"green\", \"white\", \"white\", \"red\", \"violet\"],\n",
    "]\n",
    "\n",
    "# Create cost matrix\n",
    "cost_matrix = np.zeros((rows, cols))\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        if grid[i][j] != \"gray\":\n",
    "            cost_matrix[i, j] = costs[grid[i][j]]\n",
    "        else:\n",
    "            cost_matrix[i, j] = None  # Set obstacles as None\n",
    "\n",
    "# Define possible actions (up, down, left, right, stay)\n",
    "actions = {\n",
    "    \"up\": (-1, 0),\n",
    "    \"down\": (1, 0),\n",
    "    \"left\": (0, -1),\n",
    " \"right\": (0, 1),\n",
    "    \"stay\": (0, 0)\n",
    "}\n",
    "\n",
    "# Initialize value function for each cell to zero\n",
    "value_function = np.zeros((rows, cols))\n",
    "\n",
    "def is_valid_position(row, col):\n",
    "    \"\"\"Check if a position is within bounds and not a gray cell.\"\"\"\n",
    "    return 0 <= row < rows and 0 <= col < cols and grid[row][col] != \"gray\"\n",
    "\n",
    "def value_iteration():\n",
    "    \"\"\"Perform value iteration to find the optimal value function and policy.\"\"\"\n",
    "    global value_function\n",
    "    iteration = 0\n",
    "\n",
    "    # To store the best action at each position\n",
    "    policy = np.empty((rows, cols), dtype=object)\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_value_function = value_function.copy()\n",
    "\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                if grid[i][j] == \"gray\":\n",
    "                    continue\n",
    "                \n",
    "                # Compute value for each action\n",
    "                action_values = []\n",
    "                for action, (di, dj) in actions.items():\n",
    "                    ni, nj = i + di, j + dj\n",
    "                    if is_valid_position(ni, nj):\n",
    "                        action_value = cost_matrix[i, j] + alpha * value_function[ni, nj]\n",
    "                    else:\n",
    "                        action_value = cost_matrix[i, j] + alpha * value_function[i, j]  # Stay in place if action is invalid\n",
    "                    action_values.append((action_value, action))\n",
    "\n",
    "                # Update value function for cell (i, j)\n",
    "                new_value_function[i, j] = min(action_values, key=lambda x: x[0])[0]\n",
    "\n",
    "                # Store the best action\n",
    "                best_action_value, best_action = min(action_values, key=lambda x: x[0])\n",
    "                \n",
    "                # If value is less than -99.999 ( that is 100 ), set the action to 'stay'\n",
    "                if best_action_value < -99.999:\n",
    "                    best_action = \"stay\"\n",
    "\n",
    "                # Assign the best action based on the value\n",
    "                policy[i, j] = best_action\n",
    "\n",
    "                delta = max(delta, abs(new_value_function[i, j] - value_function[i, j]))\n",
    "\n",
    "        value_function = new_value_function\n",
    "        iteration += 1\n",
    "\n",
    "        if delta < convergence_threshold:\n",
    "            break\n",
    "\n",
    "    return iteration, policy\n",
    "\n",
    "# Perform Value Iteration\n",
    "vi_iterations, final_policy = value_iteration()\n",
    "\n",
    "# Adjust the final value function according to the specified changes\n",
    "final_value_function = np.where(value_function < -99.99999, 'stay', value_function)\n",
    "final_value_function[final_value_function == 0] = None\n",
    "\n",
    "# Print final results\n",
    "print(\"Value Iteration Converged in:\", vi_iterations, \"iterations\")\n",
    "print(\"\\nFinal Value Function after Value Iteration (Cost-to-Go for Each Cell):\")\n",
    "print(final_value_function)\n",
    "\n",
    "print(\"\\nFinal Policy (Best Action at Each Cell):\")\n",
    "print(final_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee04c08f-b238-455a-90f1-7b9d62b3b6cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Value Function:\n",
      "[[ -95.08   -96.04   -97.01   -99.    -100.   ]\n",
      " [ -94.129      nan  -96.03   -98.01   -89.   ]\n",
      " [ -93.187  -82.256      nan  -97.01   -99.   ]\n",
      " [ -91.256  -90.343  -89.44   -89.    -100.   ]]\n",
      "\n",
      "Number of iterations:\n",
      "11\n",
      "\n",
      "Action Matrix (Optimal Actions):\n",
      "[['right' 'right' 'right' 'right' 'stay']\n",
      " ['up' None 'right' 'up' 'up']\n",
      " ['up' 'left' None 'right' 'down']\n",
      " ['up' 'left' 'left' 'right' 'stay']]\n"
     ]
    }
   ],
   "source": [
    "#2 B)\n",
    "\n",
    "###ANSWER\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define constants and parameters\n",
    "ROWS, COLS = 4, 5\n",
    "ALPHA = 0.99  # Discount factor\n",
    "CONVERGENCE_THRESHOLD = 1e-6  # Convergence threshold\n",
    "\n",
    "# Define costs based on cell types\n",
    "costs = {\n",
    "    \"violet\": -1,\n",
    "    \"white\": 0,\n",
    "    \"green\": 1,\n",
    "    \"red\": 10,\n",
    "    \"gray\": None  # gray cells represent obstacles\n",
    "}\n",
    "\n",
    "# Define the grid layout\n",
    "grid = [\n",
    "    [\"white\", \"white\", \"green\", \"white\", \"violet\"],\n",
    "    [\"white\", \"gray\", \"green\", \"white\", \"red\"],\n",
    "    [\"white\", \"red\", \"gray\", \"green\", \"white\"],\n",
    "    [\"green\", \"white\", \"white\", \"red\", \"violet\"],\n",
    "]\n",
    "\n",
    "# Create cost matrix\n",
    "def create_cost_matrix(grid):\n",
    "    \"\"\"Convert grid with cell colors to a numerical cost matrix.\"\"\"\n",
    "    cost_matrix = np.full((ROWS, COLS), np.nan)  # Initialize with np.nan for obstacles\n",
    "    for i in range(ROWS):\n",
    "        for j in range(COLS):\n",
    "            if grid[i][j] != \"gray\":\n",
    "                cost_matrix[i, j] = costs[grid[i][j]]\n",
    "    return cost_matrix\n",
    "\n",
    "# Action definitions\n",
    "actions = {\n",
    "    \"up\": (-1, 0),\n",
    "    \"down\": (1, 0),\n",
    "    \"left\": (0, -1),\n",
    "    \"right\": (0, 1),\n",
    "    \"stay\": (0, 0),\n",
    "}\n",
    "\n",
    "def is_valid_position(row, col):\n",
    "    \"\"\"Check if the position is within bounds and not an obstacle.\"\"\"\n",
    "    return 0 <= row < ROWS and 0 <= col < COLS and grid[row][col] != \"gray\"\n",
    "\n",
    "def initialize_A(cost_matrix):\n",
    "    \"\"\"Initialize the A matrix based on the valid positions.\"\"\"\n",
    "    num_states = np.sum(~np.isnan(cost_matrix))  # Count valid states\n",
    "    A = np.zeros((num_states, num_states))\n",
    "    return A\n",
    "\n",
    "def update_A(A, value_function, index):\n",
    "    \"\"\"Update the A matrix row for the current state.\"\"\"\n",
    "    j, k = index\n",
    "    J_nothing = value_function[j, k]\n",
    "    J_values = {\"stay\": J_nothing}\n",
    "\n",
    "    # Calculate values for each action\n",
    "    for action, (di, dj) in actions.items():\n",
    "        ni, nj = j + di, k + dj\n",
    "        if is_valid_position(ni, nj):\n",
    "            J_values[action] = value_function[ni, nj]\n",
    "        else:\n",
    "            J_values[action] = J_nothing  # If action is invalid, stay\n",
    "\n",
    "    # Find best action\n",
    "    min_cost_action = min(J_values, key=J_values.get)\n",
    "    action_representation = np.zeros((ROWS, COLS))\n",
    "\n",
    "    if min_cost_action == \"stay\":\n",
    "        action_representation[j, k] = 1\n",
    "    else:\n",
    "        ni, nj = (j + actions[min_cost_action][0]), (k + actions[min_cost_action][1])\n",
    "        action_representation[ni, nj] = 1\n",
    "\n",
    "    non_nan_idx = np.flatnonzero(~np.isnan(cost_matrix))  # Get non-NaN indices\n",
    "    A[np.where(non_nan_idx == np.ravel_multi_index((j, k), cost_matrix.shape))[0][0]] = action_representation.flatten()[non_nan_idx]\n",
    "\n",
    "def grid_world_policy_iter(max_iter, tol, alpha, g):\n",
    "    \"\"\"Perform policy iteration on the grid-world to compute the optimal value function.\"\"\"\n",
    "    iterations = np.zeros(max_iter + 1)  # Renamed this variable\n",
    "    mask = ~np.isnan(g)\n",
    "    \n",
    "    # Initialize A\n",
    "    A = initialize_A(g)\n",
    "    I = np.eye(np.sum(mask))\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # Solve for J_mu using current policy\n",
    "        J_mu = np.linalg.solve(I - alpha * A, g[mask])\n",
    "        \n",
    "        # Reshape J_mu to original grid\n",
    "        J_full = np.full(g.shape, np.nan)\n",
    "        J_full[mask] = J_mu\n",
    "        \n",
    "        # Compute the norm of the value function\n",
    "        iterations[i] = np.nanmean(J_full**2)  # Use mean to avoid NaN issues\n",
    "        \n",
    "        # Ensure the optimal policy has only positive values\n",
    "        if np.all(J_full >= 0):\n",
    "            break\n",
    "        \n",
    "        # Check for convergence\n",
    "        if i > 0 and abs(iterations[i] - iterations[i - 1]) < tol:\n",
    "            break\n",
    "        \n",
    "        # Reset A for the next iteration\n",
    "        A = initialize_A(g)\n",
    "        \n",
    "        # Update matrix A with new state evaluations\n",
    "        for j, k in np.ndindex(g.shape):\n",
    "            if not np.isnan(J_full[j, k]):\n",
    "                update_A(A, J_full, (j, k))\n",
    "\n",
    "    # Truncate the iterations history\n",
    "    iterations = iterations[:i + 1]\n",
    "    \n",
    "    return J_full, iterations\n",
    "\n",
    "# Running the policy iteration\n",
    "cost_matrix = create_cost_matrix(grid)\n",
    "optimal_values, convergence_history = grid_world_policy_iter(max_iter=1000, tol=1e-5, alpha=ALPHA, g=cost_matrix)\n",
    "\n",
    "# Print outputs\n",
    "print(\"\\nOptimal Value Function:\")\n",
    "print(optimal_values)\n",
    "\n",
    "# Number of iterations\n",
    "print(\"\\nNumber of iterations:\")\n",
    "print(len(convergence_history))\n",
    "\n",
    "# Generate action matrix based on optimal values\n",
    "def generate_action_matrix(optimal_values):\n",
    "    action_matrix = np.full((ROWS, COLS), None)  # Fill with None initially\n",
    "    \n",
    "    for j, k in np.ndindex(optimal_values.shape):\n",
    "        if not np.isnan(optimal_values[j, k]):\n",
    "            # Set action to 'stay' if value is near to -100\n",
    "            if optimal_values[j, k] <= -99.999:\n",
    "                action_matrix[j, k] = 'stay'\n",
    "                continue\n",
    "            \n",
    "            J_values = {}\n",
    "            for action, (di, dj) in actions.items():\n",
    "                ni, nj = j + di, k + dj\n",
    "                if is_valid_position(ni, nj):\n",
    "                    J_values[action] = optimal_values[ni, nj]\n",
    "                else:\n",
    "                    J_values[action] = optimal_values[j, k]  # Stay if action is invalid\n",
    "\n",
    "            best_action = min(J_values, key=J_values.get)\n",
    "            action_matrix[j, k] = best_action  # Store the best action\n",
    "    \n",
    "    return action_matrix\n",
    "\n",
    "# Generate and print the action matrix\n",
    "action_matrix = generate_action_matrix(optimal_values)\n",
    "\n",
    "print(\"\\nAction Matrix (Optimal Actions):\")\n",
    "print(action_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866eeff1-3e8c-4367-bfd8-8edf9a0bf239",
   "metadata": {},
   "source": [
    "# 2 C) ANSWER\n",
    "\n",
    "# Comparison of Value Iteration and Policy Iteration\n",
    "\n",
    "## 1. Solution Comparison\n",
    "\n",
    "### Value Iteration\n",
    "- **Goal**: Computes the optimal value function directly through iterative updates.\n",
    "- **Final Output**: Produces an optimal value function representing the expected cost-to-go for each state along with a policy (the best action for each state).\n",
    "- **Best Action**: The action corresponding to the minimum expected cost at each state after computing the value function.\n",
    "\n",
    "### Policy Iteration\n",
    "- **Goal**: Alternates between policy evaluation (calculating the value function based on the current policy) and policy improvement (updating the policy based on the newly found value function).\n",
    "- **Final Output**: Also yields an optimal value function and an optimal policy, but it often requires fewer iterations to converge compared to value iteration.\n",
    "- **Action Matrix**: The actions represent the best movement for each state, derived from the value function across iterations.\n",
    "\n",
    "## 2. Convergence Characteristics\n",
    "\n",
    "### Value Iteration\n",
    "- **Convergence**: \n",
    "  - The method converges when the maximum change in values across all states is less than a specified threshold (in this case, `1e-6`).\n",
    "  - Typically, value iteration requires a significant number of iterations, as it recalculates values for all states based on the current values for neighboring states at each iteration.\n",
    "- **Performance Considerations**:\n",
    "  - If the discount factor is close to \\(1\\) (here \\(0.99\\)), convergence may take longer, as values change slowly.\n",
    "  \n",
    "### Policy Iteration\n",
    "- **Convergence**: \n",
    "  - Often converges faster than value iteration because it improves the policy iteratively while evaluating it in the same iteration.\n",
    "  - Stops when the policy does not change during an iteration, indicating the optimal policy has been found.\n",
    "- **Performance Considerations**:\n",
    "  - Fewer iterations are generally needed, making policy iteration potentially more efficient for large state spaces, especially when the policies stabilize quickly.\n",
    "\n",
    "## 3. Computational Complexity\n",
    "\n",
    "### Value Iteration\n",
    "- **Time Complexity**: \\(O(n^2 \\cdot k)\\), where \\(n\\) is the number of states, and \\(k\\) is the number of iterations until convergence.\n",
    "- **Space Complexity**: \\(O(n)\\), as the value function and policy are stored in arrays proportional to the number of states.\n",
    "\n",
    "### Policy Iteration\n",
    "- **Time Complexity**: Although each policy evaluation can also be \\(O(n^2)\\) (solving a linear system), the overall complexity can be substantially lower in practice due to fewer iterations. If there are \\(m\\) iterations until convergence for the policy improvement, it may be roughly \\(O(m \\cdot n^3)\\) due to the linear system solver.\n",
    "- **Space Complexity**: Also \\(O(n)\\) for storing the value function and policy.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
